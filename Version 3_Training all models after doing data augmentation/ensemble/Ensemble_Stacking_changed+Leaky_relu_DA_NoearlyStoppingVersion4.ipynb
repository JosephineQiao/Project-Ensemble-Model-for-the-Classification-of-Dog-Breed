{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FC3yiTZjvsOe","outputId":"9db53079-6067-40af-b272-face1e680107","executionInfo":{"status":"ok","timestamp":1680401649519,"user_tz":-480,"elapsed":41192,"user":{"displayName":"Danyette tynan","userId":"09669877233726583532"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","\n","\n","import zipfile\n","with zipfile.ZipFile('/content/drive/MyDrive/120dog breeds-299.zip', 'r') as zip_ref:\n","    zip_ref.extractall('/content/projectDataset299')\n","\n","\n","with zipfile.ZipFile('/content/drive/MyDrive/120dog breeds-224.zip', 'r') as zip_ref:\n","    zip_ref.extractall('/content/projectDataset224')\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1kg2eDE2voQd"},"outputs":[],"source":["import pandas as pd\n","import pathlib\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","from tensorflow import keras\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.models import Sequential\n","from keras import applications\n","from keras.layers import Activation, Dropout, Flatten, Dense,GlobalAveragePooling2D, BatchNormalization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s_c-QNsKvoQd"},"outputs":[],"source":["import pathlib\n","from keras.preprocessing.image import ImageDataGenerator\n","\n","\n","data_dir1 = pathlib.Path('/content/projectDataset299/120dog breeds-299')\n","\n","data_train1 = data_dir1 / 'train'\n","\n","test_dir1 = data_dir1 / 'test'\n","\n","\n","data_dir = pathlib.Path('/content/projectDataset224/120dog breeds-224')\n","\n","data_train = data_dir / 'train'\n","\n","test_dir = data_dir / 'test'\n","\n","labels_csv = pd.read_csv('/content/drive/MyDrive/labels.csv')\n","labels_csv['id'] = [str(fname) + '.jpg' for fname in labels_csv['id']]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UHDW5BgAvoQe"},"outputs":[],"source":["batch_size = 16\n","img_size = 224\n","img_size1 = 299"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DDuxMYKVvoQe"},"outputs":[],"source":["# datagen1 = ImageDataGenerator(preprocessing_function = keras.applications.vgg16.preprocess_input, validation_split=0.2)\n","# datagen2 = ImageDataGenerator(preprocessing_function = keras.applications.resnet50.preprocess_input, validation_split=0.2)\n","# datagen3 = ImageDataGenerator(preprocessing_function = keras.applications.inception_v3.preprocess_input, validation_split=0.2)\n","from keras.utils import to_categorical\n","datagen1 = ImageDataGenerator(\n","    preprocessing_function=keras.applications.vgg16.preprocess_input,\n","    rotation_range=5,\n","    width_shift_range=0.1,\n","    height_shift_range=0.1,\n","    zoom_range=0.1,\n","    shear_range=0.05,\n","    brightness_range=[0.8, 1.2],\n","    horizontal_flip=True,\n","    validation_split=0.2\n",")\n","\n","datagen2 = ImageDataGenerator(\n","    preprocessing_function=keras.applications.resnet50.preprocess_input,\n","    rotation_range=5,\n","    width_shift_range=0.1,\n","    height_shift_range=0.1,\n","    zoom_range=0.1,\n","    shear_range=0.05,\n","    brightness_range=[0.8, 1.2],\n","    horizontal_flip=True,\n","    validation_split=0.2\n",")\n","\n","datagen3 = ImageDataGenerator(\n","    preprocessing_function=keras.applications.inception_v3.preprocess_input,\n","    rotation_range=5,\n","    width_shift_range=0.1,\n","    height_shift_range=0.1,\n","    zoom_range=0.1,\n","    shear_range=0.05,\n","    brightness_range=[0.8, 1.2],\n","    horizontal_flip=True,\n","    validation_split=0.2\n",")\n","\n","validation_datagen1 = ImageDataGenerator(\n","    preprocessing_function=keras.applications.vgg16.preprocess_input,\n","    validation_split=0.2\n",")\n","\n","validation_datagen2 = ImageDataGenerator(\n","    preprocessing_function=keras.applications.resnet50.preprocess_input,\n","    validation_split=0.2\n",")\n","\n","validation_datagen3 = ImageDataGenerator(\n","    preprocessing_function=keras.applications.inception_v3.preprocess_input,\n","    validation_split=0.2\n",")\n","def generate_generator_multiple():\n","    genX1 = datagen3.flow_from_dataframe(\n","                        dataframe = labels_csv,\n","                        directory = data_train1,\n","                        subset=\"training\",\n","                        x_col=\"id\",\n","                        y_col=\"breed\",\n","                        batch_size=batch_size,\n","                        shuffle=True,\n","                        # class_mode=\"sparse\",\n","                        class_mode='categorical',\n","                        color_mode=\"rgb\",\n","                        target_size=(img_size1, img_size1))\n","\n","    genX2 = datagen3.flow_from_dataframe(\n","                        dataframe = labels_csv,\n","                        directory = data_train1,\n","                        subset=\"training\",\n","                        x_col=\"id\",\n","                        y_col=\"breed\",\n","                        batch_size=batch_size,\n","                        shuffle=True,\n","                        # class_mode=\"sparse\",\n","                        class_mode='categorical',\n","                        color_mode=\"rgb\",\n","                        target_size=(img_size1, img_size1))\n","\n","    genX3 = datagen3.flow_from_dataframe(\n","                        dataframe = labels_csv,\n","                        directory = data_train1,\n","                        subset=\"training\",\n","                        x_col=\"id\",\n","                        y_col=\"breed\",\n","                        batch_size=batch_size,\n","                        shuffle=True,\n","                        # class_mode=\"sparse\",\n","                        class_mode='categorical',\n","                        color_mode=\"rgb\",\n","                        target_size=(img_size1, img_size1))\n","    while True:\n","            X1i = genX1.next()\n","            X2i = genX2.next()\n","            X3i = genX3.next()\n","            yield [X1i[0], X2i[0],X3i[0]], X3i[1]  #Yield both images and their mutual label\n","\n","def generate_generator_multiple2():\n","    genX1 = validation_datagen3.flow_from_dataframe(\n","                        dataframe = labels_csv,\n","                        directory = data_train1,\n","                        # subset=\"training\",\n","                        subset='validation',\n","                        x_col=\"id\",\n","                        y_col=\"breed\",\n","                        batch_size=batch_size,\n","                        shuffle=True,\n","                        # class_mode=\"sparse\",\n","                        class_mode='categorical',\n","                        color_mode=\"rgb\",\n","                        target_size=(img_size1, img_size1))\n","\n","    genX2 = validation_datagen3.flow_from_dataframe(\n","                        dataframe = labels_csv,\n","                        directory = data_train1,\n","                        # subset=\"training\",\n","                        subset='validation',\n","                        x_col=\"id\",\n","                        y_col=\"breed\",\n","                        batch_size=batch_size,\n","                        shuffle=True,\n","                        # class_mode=\"sparse\",\n","                        class_mode='categorical',\n","                        color_mode=\"rgb\",\n","                        target_size=(img_size1, img_size1))\n","\n","    genX3 = validation_datagen3.flow_from_dataframe(\n","                        dataframe = labels_csv,\n","                        directory = data_train1,\n","                        # subset=\"training\",\n","                        subset='validation',\n","                        x_col=\"id\",\n","                        y_col=\"breed\",\n","                        batch_size=batch_size,\n","                        shuffle=True,\n","                        # class_mode=\"sparse\",\n","                        class_mode='categorical',\n","                        color_mode=\"rgb\",\n","                        target_size=(img_size1, img_size1))\n","    while True:\n","            X1i = genX1.next()\n","            X2i = genX2.next()\n","            X3i = genX3.next()\n","            yield [X1i[0], X2i[0],X3i[0]], X3i[1]  #Yield both images and their mutual label\n","\n","inputgenerator=generate_generator_multiple()\n","validation_generator=generate_generator_multiple()\n","\n","\n","# train_labels = to_categorical(inputgenerator.labels)\n","# validation_labels = to_categorical(validation_generator.labels)\n"]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.python.keras.metrics import AUC\n","from sklearn.metrics import roc_curve, auc\n","\n","# Define the ROC metrics\n","tpr = tf.keras.metrics.TruePositives(name='tp')\n","fpr = tf.keras.metrics.FalsePositives(name='fp')\n","auc_metric = AUC(name='auc')"],"metadata":{"id":"XWq6hD2NJ4Za"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FgFTbPdlvoQf"},"outputs":[],"source":["class JoinedGen(tf.keras.utils.Sequence):\n","    def __init__(self, input_gen1, input_gen2, input_gen3, target_gen):\n","        self.gen1 = input_gen1\n","        self.gen2 = input_gen2\n","        self.gen3 = input_gen3\n","        self.gen4 = target_gen\n","\n","        assert len(input_gen1) == len(input_gen2) == len(input_gen3) == len(target_gen)\n","\n","    def __len__(self):\n","        return len(self.gen1)\n","\n","    def __getitem__(self, i):\n","        x1 = self.gen1[i]\n","        x2 = self.gen2[i]\n","        x3 = self.gen3[i]\n","        y = self.gen4[i]\n","\n","        return [x1, x2, x3],y\n","\n","    def on_epoch_end(self):\n","        self.gen1.on_epoch_end()\n","        self.gen2.on_epoch_end()\n","        self.gen3.on_epoch_end()\n","        self.gen4.on_epoch_end()\n","        self.gen2.index_array = self.gen1.index_array\n","        self.gen3.index_array = self.gen1.index_array\n","        self.gen4.index_array = self.gen1.index_array"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yK-vUmDuvoQf"},"outputs":[],"source":["vgg16 = applications.vgg16.VGG16(include_top=False, weights='imagenet',input_shape=(224,224,3))\n","resnet50 = keras.applications.ResNet50(include_top=False, weights='imagenet',input_shape=(224,224,3))\n","inceptionV3 = applications.inception_v3.InceptionV3(include_top=False, weights='imagenet',input_shape=(299,299,3))\n","\n","for layer in vgg16 .layers:\n","    layer.trainable=False\n","for layer in resnet50.layers:\n","    layer.trainable=False\n","for layer in inceptionV3.layers:\n","    layer.trainable=False\n","\n","model1 = Sequential()\n","model2 = Sequential()\n","model3 = Sequential() \n","\n","model1.add(vgg16)\n","model1.add(BatchNormalization())\n","model1.add(GlobalAveragePooling2D())\n","model1.add(Dropout(0.5))\n","model1.add(Dense(1024, activation='leaky_relu'))\n","model1.add(Dropout(0.5))\n","model1.add(Dense(256, activation='leaky_relu'))\n","model1.add(Dropout(0.5))\n","model1.add(Dense(120, activation='softmax'))\n","\n","model2.add(resnet50)\n","model2.add(BatchNormalization())\n","model2.add(GlobalAveragePooling2D())\n","model2.add(Dropout(0.5))\n","model2.add(Dense(1024, activation='leaky_relu'))\n","model2.add(Dropout(0.5))\n","model2.add(Dense(120, activation='softmax'))\n","\n","model3.add(inceptionV3)\n","model3.add(BatchNormalization())\n","model3.add(GlobalAveragePooling2D())\n","model3.add(Dropout(0.5))\n","model3.add(Dense(1024, activation='leaky_relu'))\n","model3.add(Dropout(0.5))\n","model3.add(Dense(120, activation='softmax'))\n","\n","optimizer=tf.keras.optimizers.Adam(learning_rate = 0.0001)\n","model1.compile(optimizer=optimizer,\n","              #  loss='sparse_categorical_crossentropy',\n","              loss='categorical_crossentropy',\n","              #  metrics=['accuracy'])\n","              metrics=[tpr, fpr, auc_metric, 'accuracy', 'Precision', 'Recall'])\n","model2.compile(optimizer=optimizer,\n","              #  loss='sparse_categorical_crossentropy',\n","              loss='categorical_crossentropy',\n","              #  metrics=['accuracy'])\n","              metrics=[tpr, fpr, auc_metric, 'accuracy', 'Precision', 'Recall'])\n","model3.compile(optimizer=optimizer,\n","              #  loss='sparse_categorical_crossentropy',\n","              loss='categorical_crossentropy',\n","              #  metrics=['accuracy'])\n","              metrics=[tpr, fpr, auc_metric, 'accuracy', 'Precision', 'Recall'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":403,"status":"ok","timestamp":1680417351431,"user":{"displayName":"Danyette tynan","userId":"09669877233726583532"},"user_tz":-480},"id":"jgSoQ4LVvoQg","outputId":"d40f981b-7a81-4be3-aafe-8a105e373b26"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_3\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," vgg16_input (InputLayer)       [(None, 224, 224, 3  0           []                               \n","                                )]                                                                \n","                                                                                                  \n"," vgg16 (Functional)             (None, 7, 7, 512)    14714688    ['vgg16_input[0][0]']            \n","                                                                                                  \n"," batch_normalization_291 (Batch  (None, 7, 7, 512)   2048        ['vgg16[0][0]']                  \n"," Normalization)                                                                                   \n","                                                                                                  \n"," resnet50_input (InputLayer)    [(None, 224, 224, 3  0           []                               \n","                                )]                                                                \n","                                                                                                  \n"," inception_v3_input (InputLayer  [(None, 299, 299, 3  0          []                               \n"," )                              )]                                                                \n","                                                                                                  \n"," global_average_pooling2d_6 (Gl  (None, 512)         0           ['batch_normalization_291[0][0]']\n"," obalAveragePooling2D)                                                                            \n","                                                                                                  \n"," resnet50 (Functional)          (None, 7, 7, 2048)   23587712    ['resnet50_input[0][0]']         \n","                                                                                                  \n"," inception_v3 (Functional)      (None, 8, 8, 2048)   21802784    ['inception_v3_input[0][0]']     \n","                                                                                                  \n"," dropout_17 (Dropout)           (None, 512)          0           ['global_average_pooling2d_6[0][0\n","                                                                 ]']                              \n","                                                                                                  \n"," batch_normalization_292 (Batch  (None, 7, 7, 2048)  8192        ['resnet50[0][0]']               \n"," Normalization)                                                                                   \n","                                                                                                  \n"," batch_normalization_293 (Batch  (None, 8, 8, 2048)  8192        ['inception_v3[0][0]']           \n"," Normalization)                                                                                   \n","                                                                                                  \n"," dense_20 (Dense)               (None, 1024)         525312      ['dropout_17[0][0]']             \n","                                                                                                  \n"," global_average_pooling2d_7 (Gl  (None, 2048)        0           ['batch_normalization_292[0][0]']\n"," obalAveragePooling2D)                                                                            \n","                                                                                                  \n"," global_average_pooling2d_8 (Gl  (None, 2048)        0           ['batch_normalization_293[0][0]']\n"," obalAveragePooling2D)                                                                            \n","                                                                                                  \n"," dropout_18 (Dropout)           (None, 1024)         0           ['dense_20[0][0]']               \n","                                                                                                  \n"," dropout_20 (Dropout)           (None, 2048)         0           ['global_average_pooling2d_7[0][0\n","                                                                 ]']                              \n","                                                                                                  \n"," dropout_22 (Dropout)           (None, 2048)         0           ['global_average_pooling2d_8[0][0\n","                                                                 ]']                              \n","                                                                                                  \n"," dense_21 (Dense)               (None, 256)          262400      ['dropout_18[0][0]']             \n","                                                                                                  \n"," dense_23 (Dense)               (None, 1024)         2098176     ['dropout_20[0][0]']             \n","                                                                                                  \n"," dense_25 (Dense)               (None, 1024)         2098176     ['dropout_22[0][0]']             \n","                                                                                                  \n"," dropout_19 (Dropout)           (None, 256)          0           ['dense_21[0][0]']               \n","                                                                                                  \n"," dropout_21 (Dropout)           (None, 1024)         0           ['dense_23[0][0]']               \n","                                                                                                  \n"," dropout_23 (Dropout)           (None, 1024)         0           ['dense_25[0][0]']               \n","                                                                                                  \n"," dense_22 (Dense)               (None, 120)          30840       ['dropout_19[0][0]']             \n","                                                                                                  \n"," dense_24 (Dense)               (None, 120)          123000      ['dropout_21[0][0]']             \n","                                                                                                  \n"," dense_26 (Dense)               (None, 120)          123000      ['dropout_23[0][0]']             \n","                                                                                                  \n"," concatenate_9 (Concatenate)    (None, 360)          0           ['dense_22[0][0]',               \n","                                                                  'dense_24[0][0]',               \n","                                                                  'dense_26[0][0]']               \n","                                                                                                  \n"," batch_normalization_294 (Batch  (None, 360)         1440        ['concatenate_9[0][0]']          \n"," Normalization)                                                                                   \n","                                                                                                  \n"," dense_27 (Dense)               (None, 1024)         369664      ['batch_normalization_294[0][0]']\n","                                                                                                  \n"," dropout_24 (Dropout)           (None, 1024)         0           ['dense_27[0][0]']               \n","                                                                                                  \n"," dense_28 (Dense)               (None, 120)          123000      ['dropout_24[0][0]']             \n","                                                                                                  \n","==================================================================================================\n","Total params: 65,878,624\n","Trainable params: 5,763,504\n","Non-trainable params: 60,115,120\n","__________________________________________________________________________________________________\n"]}],"source":["models = []\n","models.append(model1)\n","models.append(model2)\n","models.append(model3)\n","ensemble_visible = [model.input for model in models]\n","ensemble_outputs = [model.output for model in models]\n","merge = tf.keras.layers.concatenate(ensemble_outputs)\n","merge = tf.keras.layers.BatchNormalization()(merge)\n","merge = tf.keras.layers.Dense(1024, activation='relu')(merge)\n","merge = tf.keras.layers.Dropout(0.5)(merge)\n","output = tf.keras.layers.Dense(120, activation='softmax')(merge)\n","ensemble_model = tf.keras.models.Model(inputs=ensemble_visible, outputs=output)\n","ensemble_model.compile(optimizer=optimizer,\n","                      #  loss='sparse_categorical_crossentropy',\n","                      loss='categorical_crossentropy',\n","                      #  metrics=['accuracy'])\n","                      metrics=[tpr, fpr, auc_metric, 'accuracy', 'Precision', 'Recall'])\n","ensemble_model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pbgDMnMCvoQh","outputId":"0151cb22-8394-4887-e823-6883309ae1b6","scrolled":false},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 8178 validated image filenames belonging to 120 classes.\n","Found 8178 validated image filenames belonging to 120 classes.\n","Found 8178 validated image filenames belonging to 120 classes.\n","Epoch 1/20\n","512/511 [==============================] - ETA: 0s - loss: 4.5004 - tp: 0.0000e+00 - fp: 0.0000e+00 - auc: 0.5838 - accuracy: 0.1242 - precision: 0.0000e+00 - recall: 0.0000e+00Found 8178 validated image filenames belonging to 120 classes.\n","Found 8178 validated image filenames belonging to 120 classes.\n","Found 8178 validated image filenames belonging to 120 classes.\n","511/511 [==============================] - 803s 2s/step - loss: 4.5004 - tp: 0.0000e+00 - fp: 0.0000e+00 - auc: 0.5838 - accuracy: 0.1242 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 4.0157 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_auc: 0.7230 - val_accuracy: 0.4668 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n","Epoch 2/20\n","511/511 [==============================] - 781s 2s/step - loss: 3.0249 - tp: 99.0000 - fp: 14.0000 - auc: 0.8063 - accuracy: 0.4467 - precision: 0.8761 - recall: 0.0121 - val_loss: 1.2916 - val_tp: 722.0000 - val_fp: 118.0000 - val_auc: 0.8677 - val_accuracy: 0.6943 - val_precision: 0.8595 - val_recall: 0.3525\n","Epoch 3/20\n"," 95/511 [====>.........................] - ETA: 7:54 - loss: 1.9236 - tp: 148.0000 - fp: 21.0000 - auc: 0.8836 - accuracy: 0.5882 - precision: 0.8757 - recall: 0.0974"]}],"source":["history_ens = ensemble_model.fit(inputgenerator, epochs=20, verbose = 1,\n","                                 steps_per_epoch = 8178/batch_size,\n","                                 validation_steps = 2044/batch_size,\n","                                 validation_data=validation_generator)\n","\n","# from keras.callbacks import ModelCheckpoint, EarlyStopping\n","\n","\n","# checkpoint_cb = ModelCheckpoint(\"/content/drive/MyDrive/Ensemble_StackingVersion3.h5\", save_best_only=True)\n","# early_stopping_cb = EarlyStopping(patience=5, restore_best_weights=True)\n","\n","# history_ens = ensemble_model.fit(inputgenerator, epochs=50, verbose = 1,\n","#                         steps_per_epoch = 8178/batch_size,\n","#                         validation_steps = 2044/batch_size,\n","#                         validation_data=validation_generator,\n","#                         callbacks=[checkpoint_cb, early_stopping_cb])"]},{"cell_type":"code","source":["ensemble_model.save('/content/drive/MyDrive/Ensemble_StackingVersion4.h5')"],"metadata":{"id":"cEjVwxztCSB7"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8N6AcgErvoQh"},"outputs":[],"source":["plt.figure()\n","plt.plot(history_ens .history[\"val_loss\"], label = \"Val loss\")\n","plt.plot(history_ens .history[\"loss\"], label = \"Train loss\")\n","plt.legend()\n","plt.show()\n","plt.figure()\n","plt.plot(history_ens .history[\"val_accuracy\"], label = \"Val accuracy\")\n","plt.plot(history_ens .history[\"accuracy\"], label = \"Accuracy\")\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","source":["plt.figure()\n","plt.plot(history_ens .history['val_auc'], label = 'val_auc')\n","plt.plot(history_ens .history['auc'], label = 'Train auc')\n","plt.legend()\n","plt.show()\n","\n","plt.figure()\n","plt.plot(history_ens .history['val_tp'], label = 'Val TP')\n","plt.plot(history_ens .history['tp'], label = 'Train TP')\n","plt.plot(history_ens .history['val_fp'], label = 'Val FP')\n","plt.plot(history_ens .history['fp'], label = 'Train FP')\n","plt.legend()\n","plt.show()\n","\n","plt.figure()\n","plt.plot(history_ens .history['val_precision'], label = 'Val Precision')\n","plt.plot(history_ens .history['precision'], label = 'Train Precision')\n","plt.legend()\n","plt.show()\n","\n","plt.figure()\n","plt.plot(history_ens .history['val_recall'], label = 'Val Recall')\n","plt.plot(history_ens .history['recall'], label = 'Train Recall')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"nLNFpuGjVcDj"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1XSeBgGh2gbQCyCLJRRJOmHgPAyPXdS9f","timestamp":1680335055755},{"file_id":"1N9Rvz9GzyY2YLa0jrUvTRXngrJXOCXn4","timestamp":1680270237486}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}