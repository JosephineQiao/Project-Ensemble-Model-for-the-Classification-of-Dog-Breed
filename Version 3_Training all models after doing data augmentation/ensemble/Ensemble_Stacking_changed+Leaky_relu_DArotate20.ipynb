{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FC3yiTZjvsOe","outputId":"5930f5ce-f5a8-4bb2-f60c-0ecee8198fd1","executionInfo":{"status":"ok","timestamp":1682301152231,"user_tz":-480,"elapsed":47025,"user":{"displayName":"Danyette tynan","userId":"09669877233726583532"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","\n","\n","import zipfile\n","with zipfile.ZipFile('/content/drive/MyDrive/120dog breeds-299.zip', 'r') as zip_ref:\n","    zip_ref.extractall('/content/projectDataset299')\n","\n","\n","with zipfile.ZipFile('/content/drive/MyDrive/120dog breeds-224.zip', 'r') as zip_ref:\n","    zip_ref.extractall('/content/projectDataset224')\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1kg2eDE2voQd"},"outputs":[],"source":["import pandas as pd\n","import pathlib\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","from tensorflow import keras\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.models import Sequential\n","from keras import applications\n","from keras.layers import Activation, Dropout, Flatten, Dense,GlobalAveragePooling2D, BatchNormalization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s_c-QNsKvoQd"},"outputs":[],"source":["import pathlib\n","from keras.preprocessing.image import ImageDataGenerator\n","\n","\n","data_dir1 = pathlib.Path('/content/projectDataset299/120dog breeds-299')\n","\n","data_train1 = data_dir1 / 'train'\n","\n","test_dir1 = data_dir1 / 'test'\n","\n","\n","data_dir = pathlib.Path('/content/projectDataset224/120dog breeds-224')\n","\n","data_train = data_dir / 'train'\n","\n","test_dir = data_dir / 'test'\n","\n","labels_csv = pd.read_csv('/content/drive/MyDrive/labels.csv')\n","labels_csv['id'] = [str(fname) + '.jpg' for fname in labels_csv['id']]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UHDW5BgAvoQe"},"outputs":[],"source":["batch_size = 16\n","img_size = 224\n","img_size1 = 299"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DDuxMYKVvoQe"},"outputs":[],"source":["# datagen1 = ImageDataGenerator(preprocessing_function = keras.applications.vgg16.preprocess_input, validation_split=0.2)\n","# datagen2 = ImageDataGenerator(preprocessing_function = keras.applications.resnet50.preprocess_input, validation_split=0.2)\n","# datagen3 = ImageDataGenerator(preprocessing_function = keras.applications.inception_v3.preprocess_input, validation_split=0.2)\n","from keras.utils import to_categorical\n","datagen1 = ImageDataGenerator(\n","    preprocessing_function=keras.applications.vgg16.preprocess_input,\n","    validation_split=0.2,\n","    rotation_range=20, \n","    width_shift_range=0.2, \n","    height_shift_range=0.2, \n","    zoom_range=0.2, \n","    horizontal_flip=True, \n","    vertical_flip=True\n",")\n","\n","datagen2 = ImageDataGenerator(\n","    preprocessing_function=keras.applications.resnet50.preprocess_input,\n","    validation_split=0.2,\n","    rotation_range=20, \n","    width_shift_range=0.2, \n","    height_shift_range=0.2, \n","    zoom_range=0.2, \n","    horizontal_flip=True, \n","    vertical_flip=True\n",")\n","\n","datagen3 = ImageDataGenerator(\n","    preprocessing_function=keras.applications.inception_v3.preprocess_input,\n","    validation_split=0.2,\n","    rotation_range=20, \n","    width_shift_range=0.2, \n","    height_shift_range=0.2, \n","    zoom_range=0.2, \n","    horizontal_flip=True, \n","    vertical_flip=True\n",")\n","\n","validation_datagen1 = ImageDataGenerator(\n","    preprocessing_function=keras.applications.vgg16.preprocess_input,\n","    validation_split=0.2\n",")\n","\n","validation_datagen2 = ImageDataGenerator(\n","    preprocessing_function=keras.applications.resnet50.preprocess_input,\n","    validation_split=0.2\n",")\n","\n","validation_datagen3 = ImageDataGenerator(\n","    preprocessing_function=keras.applications.inception_v3.preprocess_input,\n","    validation_split=0.2\n",")\n","def generate_generator_multiple():\n","    genX1 = datagen3.flow_from_dataframe(\n","                        dataframe = labels_csv,\n","                        directory = data_train1,\n","                        subset=\"training\",\n","                        x_col=\"id\",\n","                        y_col=\"breed\",\n","                        batch_size=batch_size,\n","                        shuffle=True,\n","                        # class_mode=\"sparse\",\n","                        class_mode='categorical',\n","                        color_mode=\"rgb\",\n","                        target_size=(img_size1, img_size1))\n","\n","    genX2 = datagen3.flow_from_dataframe(\n","                        dataframe = labels_csv,\n","                        directory = data_train1,\n","                        subset=\"training\",\n","                        x_col=\"id\",\n","                        y_col=\"breed\",\n","                        batch_size=batch_size,\n","                        shuffle=True,\n","                        # class_mode=\"sparse\",\n","                        class_mode='categorical',\n","                        color_mode=\"rgb\",\n","                        target_size=(img_size1, img_size1))\n","\n","    genX3 = datagen3.flow_from_dataframe(\n","                        dataframe = labels_csv,\n","                        directory = data_train1,\n","                        subset=\"training\",\n","                        x_col=\"id\",\n","                        y_col=\"breed\",\n","                        batch_size=batch_size,\n","                        shuffle=True,\n","                        # class_mode=\"sparse\",\n","                        class_mode='categorical',\n","                        color_mode=\"rgb\",\n","                        target_size=(img_size1, img_size1))\n","    while True:\n","            X1i = genX1.next()\n","            X2i = genX2.next()\n","            X3i = genX3.next()\n","            yield [X1i[0], X2i[0],X3i[0]], X3i[1]  #Yield both images and their mutual label\n","\n","def generate_generator_multiple2():\n","    genX1 = validation_datagen3.flow_from_dataframe(\n","                        dataframe = labels_csv,\n","                        directory = data_train1,\n","                        # subset=\"training\",\n","                        subset='validation',\n","                        x_col=\"id\",\n","                        y_col=\"breed\",\n","                        batch_size=batch_size,\n","                        shuffle=True,\n","                        # class_mode=\"sparse\",\n","                        class_mode='categorical',\n","                        color_mode=\"rgb\",\n","                        target_size=(img_size1, img_size1))\n","\n","    genX2 = validation_datagen3.flow_from_dataframe(\n","                        dataframe = labels_csv,\n","                        directory = data_train1,\n","                        # subset=\"training\",\n","                        subset='validation',\n","                        x_col=\"id\",\n","                        y_col=\"breed\",\n","                        batch_size=batch_size,\n","                        shuffle=True,\n","                        # class_mode=\"sparse\",\n","                        class_mode='categorical',\n","                        color_mode=\"rgb\",\n","                        target_size=(img_size1, img_size1))\n","\n","    genX3 = validation_datagen3.flow_from_dataframe(\n","                        dataframe = labels_csv,\n","                        directory = data_train1,\n","                        # subset=\"training\",\n","                        subset='validation',\n","                        x_col=\"id\",\n","                        y_col=\"breed\",\n","                        batch_size=batch_size,\n","                        shuffle=True,\n","                        # class_mode=\"sparse\",\n","                        class_mode='categorical',\n","                        color_mode=\"rgb\",\n","                        target_size=(img_size1, img_size1))\n","    while True:\n","            X1i = genX1.next()\n","            X2i = genX2.next()\n","            X3i = genX3.next()\n","            yield [X1i[0], X2i[0],X3i[0]], X3i[1]  #Yield both images and their mutual label\n","\n","inputgenerator=generate_generator_multiple()\n","validation_generator=generate_generator_multiple()"]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.python.keras.metrics import AUC\n","from sklearn.metrics import roc_curve, auc\n","\n","# Define the ROC metrics\n","tpr = tf.keras.metrics.TruePositives(name='tp')\n","fpr = tf.keras.metrics.FalsePositives(name='fp')\n","auc_metric = AUC(name='auc')"],"metadata":{"id":"XWq6hD2NJ4Za"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yK-vUmDuvoQf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682301183985,"user_tz":-480,"elapsed":23863,"user":{"displayName":"Danyette tynan","userId":"09669877233726583532"}},"outputId":"74b4dab1-42e0-4ca9-df10-5b13e5de4658"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n","58889256/58889256 [==============================] - 4s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n","94765736/94765736 [==============================] - 5s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n","87910968/87910968 [==============================] - 5s 0us/step\n"]}],"source":["vgg16 = applications.vgg16.VGG16(include_top=False, weights='imagenet',input_shape=(224,224,3))\n","resnet50 = keras.applications.ResNet50(include_top=False, weights='imagenet',input_shape=(224,224,3))\n","inceptionV3 = applications.inception_v3.InceptionV3(include_top=False, weights='imagenet',input_shape=(299,299,3))\n","\n","for layer in vgg16 .layers:\n","    layer.trainable=False\n","for layer in resnet50.layers:\n","    layer.trainable=False\n","for layer in inceptionV3.layers:\n","    layer.trainable=False\n","\n","model1 = Sequential()\n","model2 = Sequential()\n","model3 = Sequential() \n","\n","model1.add(vgg16)\n","model1.add(BatchNormalization())\n","model1.add(GlobalAveragePooling2D())\n","model1.add(Dropout(0.5))\n","model1.add(Dense(1024, activation='leaky_relu'))\n","model1.add(Dropout(0.5))\n","model1.add(Dense(256, activation='leaky_relu'))\n","model1.add(Dropout(0.5))\n","model1.add(Dense(120, activation='softmax'))\n","\n","model2.add(resnet50)\n","model2.add(BatchNormalization())\n","model2.add(GlobalAveragePooling2D())\n","model2.add(Dropout(0.5))\n","model2.add(Dense(1024, activation='leaky_relu'))\n","model2.add(Dropout(0.5))\n","model2.add(Dense(120, activation='softmax'))\n","\n","model3.add(inceptionV3)\n","model3.add(BatchNormalization())\n","model3.add(GlobalAveragePooling2D())\n","model3.add(Dropout(0.5))\n","model3.add(Dense(1024, activation='leaky_relu'))\n","model3.add(Dropout(0.5))\n","model3.add(Dense(120, activation='softmax'))\n","\n","optimizer=tf.keras.optimizers.Adam(learning_rate = 0.0001)\n","model1.compile(optimizer=optimizer,\n","              #  loss='sparse_categorical_crossentropy',\n","              loss='categorical_crossentropy',\n","              #  metrics=['accuracy'])\n","              metrics=[tpr, fpr, auc_metric, 'accuracy', 'Precision', 'Recall'])\n","model2.compile(optimizer=optimizer,\n","              #  loss='sparse_categorical_crossentropy',\n","              loss='categorical_crossentropy',\n","              #  metrics=['accuracy'])\n","              metrics=[tpr, fpr, auc_metric, 'accuracy', 'Precision', 'Recall'])\n","model3.compile(optimizer=optimizer,\n","              #  loss='sparse_categorical_crossentropy',\n","              loss='categorical_crossentropy',\n","              #  metrics=['accuracy'])\n","              metrics=[tpr, fpr, auc_metric, 'accuracy', 'Precision', 'Recall'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32,"status":"ok","timestamp":1682301183986,"user":{"displayName":"Danyette tynan","userId":"09669877233726583532"},"user_tz":-480},"id":"jgSoQ4LVvoQg","outputId":"36cb3437-52d9-4670-9ffb-d9b42bc85c6f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," vgg16_input (InputLayer)       [(None, 224, 224, 3  0           []                               \n","                                )]                                                                \n","                                                                                                  \n"," vgg16 (Functional)             (None, 7, 7, 512)    14714688    ['vgg16_input[0][0]']            \n","                                                                                                  \n"," batch_normalization_94 (BatchN  (None, 7, 7, 512)   2048        ['vgg16[0][0]']                  \n"," ormalization)                                                                                    \n","                                                                                                  \n"," resnet50_input (InputLayer)    [(None, 224, 224, 3  0           []                               \n","                                )]                                                                \n","                                                                                                  \n"," inception_v3_input (InputLayer  [(None, 299, 299, 3  0          []                               \n"," )                              )]                                                                \n","                                                                                                  \n"," global_average_pooling2d (Glob  (None, 512)         0           ['batch_normalization_94[0][0]'] \n"," alAveragePooling2D)                                                                              \n","                                                                                                  \n"," resnet50 (Functional)          (None, 7, 7, 2048)   23587712    ['resnet50_input[0][0]']         \n","                                                                                                  \n"," inception_v3 (Functional)      (None, 8, 8, 2048)   21802784    ['inception_v3_input[0][0]']     \n","                                                                                                  \n"," dropout (Dropout)              (None, 512)          0           ['global_average_pooling2d[0][0]'\n","                                                                 ]                                \n","                                                                                                  \n"," batch_normalization_95 (BatchN  (None, 7, 7, 2048)  8192        ['resnet50[0][0]']               \n"," ormalization)                                                                                    \n","                                                                                                  \n"," batch_normalization_96 (BatchN  (None, 8, 8, 2048)  8192        ['inception_v3[0][0]']           \n"," ormalization)                                                                                    \n","                                                                                                  \n"," dense (Dense)                  (None, 1024)         525312      ['dropout[0][0]']                \n","                                                                                                  \n"," global_average_pooling2d_1 (Gl  (None, 2048)        0           ['batch_normalization_95[0][0]'] \n"," obalAveragePooling2D)                                                                            \n","                                                                                                  \n"," global_average_pooling2d_2 (Gl  (None, 2048)        0           ['batch_normalization_96[0][0]'] \n"," obalAveragePooling2D)                                                                            \n","                                                                                                  \n"," dropout_1 (Dropout)            (None, 1024)         0           ['dense[0][0]']                  \n","                                                                                                  \n"," dropout_3 (Dropout)            (None, 2048)         0           ['global_average_pooling2d_1[0][0\n","                                                                 ]']                              \n","                                                                                                  \n"," dropout_5 (Dropout)            (None, 2048)         0           ['global_average_pooling2d_2[0][0\n","                                                                 ]']                              \n","                                                                                                  \n"," dense_1 (Dense)                (None, 256)          262400      ['dropout_1[0][0]']              \n","                                                                                                  \n"," dense_3 (Dense)                (None, 1024)         2098176     ['dropout_3[0][0]']              \n","                                                                                                  \n"," dense_5 (Dense)                (None, 1024)         2098176     ['dropout_5[0][0]']              \n","                                                                                                  \n"," dropout_2 (Dropout)            (None, 256)          0           ['dense_1[0][0]']                \n","                                                                                                  \n"," dropout_4 (Dropout)            (None, 1024)         0           ['dense_3[0][0]']                \n","                                                                                                  \n"," dropout_6 (Dropout)            (None, 1024)         0           ['dense_5[0][0]']                \n","                                                                                                  \n"," dense_2 (Dense)                (None, 120)          30840       ['dropout_2[0][0]']              \n","                                                                                                  \n"," dense_4 (Dense)                (None, 120)          123000      ['dropout_4[0][0]']              \n","                                                                                                  \n"," dense_6 (Dense)                (None, 120)          123000      ['dropout_6[0][0]']              \n","                                                                                                  \n"," concatenate_2 (Concatenate)    (None, 360)          0           ['dense_2[0][0]',                \n","                                                                  'dense_4[0][0]',                \n","                                                                  'dense_6[0][0]']                \n","                                                                                                  \n"," batch_normalization_97 (BatchN  (None, 360)         1440        ['concatenate_2[0][0]']          \n"," ormalization)                                                                                    \n","                                                                                                  \n"," dense_7 (Dense)                (None, 1024)         369664      ['batch_normalization_97[0][0]'] \n","                                                                                                  \n"," dropout_7 (Dropout)            (None, 1024)         0           ['dense_7[0][0]']                \n","                                                                                                  \n"," dense_8 (Dense)                (None, 120)          123000      ['dropout_7[0][0]']              \n","                                                                                                  \n","==================================================================================================\n","Total params: 65,878,624\n","Trainable params: 5,763,504\n","Non-trainable params: 60,115,120\n","__________________________________________________________________________________________________\n"]}],"source":["models = []\n","models.append(model1)\n","models.append(model2)\n","models.append(model3)\n","ensemble_visible = [model.input for model in models]\n","ensemble_outputs = [model.output for model in models]\n","merge = tf.keras.layers.concatenate(ensemble_outputs)\n","merge = tf.keras.layers.BatchNormalization()(merge)\n","merge = tf.keras.layers.Dense(1024, activation='relu')(merge)\n","merge = tf.keras.layers.Dropout(0.5)(merge)\n","output = tf.keras.layers.Dense(120, activation='softmax')(merge)\n","ensemble_model = tf.keras.models.Model(inputs=ensemble_visible, outputs=output)\n","ensemble_model.compile(optimizer=optimizer,\n","                      #  loss='sparse_categorical_crossentropy',\n","                      loss='categorical_crossentropy',\n","                      #  metrics=['accuracy'])\n","                      metrics=[tpr, fpr, auc_metric, 'accuracy', 'Precision', 'Recall'])\n","ensemble_model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pbgDMnMCvoQh","outputId":"e7772781-f183-46dd-deb6-59fabf8ee357","scrolled":false},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 8178 validated image filenames belonging to 120 classes.\n","Found 8178 validated image filenames belonging to 120 classes.\n","Found 8178 validated image filenames belonging to 120 classes.\n","Epoch 1/20\n","512/511 [==============================] - ETA: 0s - loss: 4.6959 - tp: 0.0000e+00 - fp: 0.0000e+00 - auc: 0.5396 - accuracy: 0.0412 - precision: 0.0000e+00 - recall: 0.0000e+00Found 8178 validated image filenames belonging to 120 classes.\n","Found 8178 validated image filenames belonging to 120 classes.\n","Found 8178 validated image filenames belonging to 120 classes.\n","511/511 [==============================] - 724s 1s/step - loss: 4.6959 - tp: 0.0000e+00 - fp: 0.0000e+00 - auc: 0.5396 - accuracy: 0.0412 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 4.5660 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_auc: 0.5944 - val_accuracy: 0.2241 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n","Epoch 2/20\n","511/511 [==============================] - 687s 1s/step - loss: 4.0613 - tp: 1.0000 - fp: 0.0000e+00 - auc: 0.6572 - accuracy: 0.1730 - precision: 1.0000 - recall: 1.2228e-04 - val_loss: 3.0322 - val_tp: 81.0000 - val_fp: 21.0000 - val_auc: 0.7298 - val_accuracy: 0.3447 - val_precision: 0.7941 - val_recall: 0.0396\n","Epoch 3/20\n","511/511 [==============================] - 695s 1s/step - loss: 3.0664 - tp: 96.0000 - fp: 27.0000 - auc: 0.7821 - accuracy: 0.3059 - precision: 0.7805 - recall: 0.0117 - val_loss: 2.0033 - val_tp: 351.0000 - val_fp: 72.0000 - val_auc: 0.8230 - val_accuracy: 0.5005 - val_precision: 0.8298 - val_recall: 0.1714\n","Epoch 4/20\n","511/511 [==============================] - 696s 1s/step - loss: 2.3984 - tp: 654.0000 - fp: 174.0000 - auc: 0.8500 - accuracy: 0.4017 - precision: 0.7899 - recall: 0.0800 - val_loss: 1.7020 - val_tp: 642.0000 - val_fp: 170.0000 - val_auc: 0.8703 - val_accuracy: 0.5344 - val_precision: 0.7906 - val_recall: 0.3156\n","Epoch 5/20\n","511/511 [==============================] - 695s 1s/step - loss: 2.1025 - tp: 1422.0000 - fp: 434.0000 - auc: 0.8845 - accuracy: 0.4472 - precision: 0.7662 - recall: 0.1739 - val_loss: 1.5675 - val_tp: 820.0000 - val_fp: 271.0000 - val_auc: 0.8961 - val_accuracy: 0.5469 - val_precision: 0.7516 - val_recall: 0.4004\n","Epoch 6/20\n","511/511 [==============================] - 691s 1s/step - loss: 1.9888 - tp: 1997.0000 - fp: 660.0000 - auc: 0.9049 - accuracy: 0.4696 - precision: 0.7516 - recall: 0.2442 - val_loss: 1.4229 - val_tp: 935.0000 - val_fp: 274.0000 - val_auc: 0.9120 - val_accuracy: 0.5972 - val_precision: 0.7734 - val_recall: 0.4565\n","Epoch 7/20\n","511/511 [==============================] - 698s 1s/step - loss: 1.9083 - tp: 2291.0000 - fp: 728.0000 - auc: 0.9176 - accuracy: 0.4908 - precision: 0.7589 - recall: 0.2801 - val_loss: 1.3941 - val_tp: 1007.0000 - val_fp: 290.0000 - val_auc: 0.9224 - val_accuracy: 0.6133 - val_precision: 0.7764 - val_recall: 0.4917\n","Epoch 8/20\n","511/511 [==============================] - 696s 1s/step - loss: 1.8035 - tp: 2658.0000 - fp: 816.0000 - auc: 0.9265 - accuracy: 0.5160 - precision: 0.7651 - recall: 0.3250 - val_loss: 1.3400 - val_tp: 1091.0000 - val_fp: 320.0000 - val_auc: 0.9299 - val_accuracy: 0.6347 - val_precision: 0.7732 - val_recall: 0.5364\n","Epoch 9/20\n","511/511 [==============================] - 699s 1s/step - loss: 1.7670 - tp: 2887.0000 - fp: 897.0000 - auc: 0.9326 - accuracy: 0.5262 - precision: 0.7629 - recall: 0.3530 - val_loss: 1.2946 - val_tp: 1088.0000 - val_fp: 317.0000 - val_auc: 0.9352 - val_accuracy: 0.6328 - val_precision: 0.7744 - val_recall: 0.5312\n","Epoch 10/20\n","511/511 [==============================] - 703s 1s/step - loss: 1.7157 - tp: 3015.0000 - fp: 962.0000 - auc: 0.9375 - accuracy: 0.5402 - precision: 0.7581 - recall: 0.3687 - val_loss: 1.3214 - val_tp: 1105.0000 - val_fp: 317.0000 - val_auc: 0.9396 - val_accuracy: 0.6396 - val_precision: 0.7771 - val_recall: 0.5396\n","Epoch 11/20\n","511/511 [==============================] - 692s 1s/step - loss: 1.6874 - tp: 3169.0000 - fp: 943.0000 - auc: 0.9414 - accuracy: 0.5483 - precision: 0.7707 - recall: 0.3875 - val_loss: 1.2304 - val_tp: 1157.0000 - val_fp: 341.0000 - val_auc: 0.9429 - val_accuracy: 0.6548 - val_precision: 0.7724 - val_recall: 0.5649\n","Epoch 12/20\n","511/511 [==============================] - 686s 1s/step - loss: 1.6559 - tp: 3252.0000 - fp: 912.0000 - auc: 0.9443 - accuracy: 0.5576 - precision: 0.7810 - recall: 0.3977 - val_loss: 1.2389 - val_tp: 1160.0000 - val_fp: 307.0000 - val_auc: 0.9457 - val_accuracy: 0.6563 - val_precision: 0.7907 - val_recall: 0.5703\n","Epoch 13/20\n","511/511 [==============================] - 688s 1s/step - loss: 1.6255 - tp: 3451.0000 - fp: 1043.0000 - auc: 0.9468 - accuracy: 0.5657 - precision: 0.7679 - recall: 0.4220 - val_loss: 1.1766 - val_tp: 1184.0000 - val_fp: 309.0000 - val_auc: 0.9480 - val_accuracy: 0.6655 - val_precision: 0.7930 - val_recall: 0.5781\n","Epoch 14/20\n","512/511 [==============================] - ETA: 0s - loss: 1.5763 - tp: 3524.0000 - fp: 1024.0000 - auc: 0.9491 - accuracy: 0.5726 - precision: 0.7748 - recall: 0.4309"]}],"source":["history_ens = ensemble_model.fit(inputgenerator, epochs=20, verbose = 1,\n","                                 steps_per_epoch = 8178/batch_size,\n","                                 validation_steps = 2044/batch_size,\n","                                 validation_data=validation_generator)\n","\n","# from keras.callbacks import ModelCheckpoint, EarlyStopping\n","\n","\n","# checkpoint_cb = ModelCheckpoint(\"/content/drive/MyDrive/Ensemble_StackingVersion3.h5\", save_best_only=True)\n","# early_stopping_cb = EarlyStopping(patience=5, restore_best_weights=True)\n","\n","\n","# history_ens = ensemble_model.fit(inputgenerator, epochs=50, verbose = 1,\n","#                         steps_per_epoch = 8178/batch_size,\n","#                         validation_steps = 2044/batch_size,\n","#                         validation_data=validation_generator,\n","#                         callbacks=[checkpoint_cb, early_stopping_cb])"]},{"cell_type":"code","source":["ensemble_model.save('/content/drive/MyDrive/Ensemble_StackingDARotate20.h5')"],"metadata":{"id":"cEjVwxztCSB7"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8N6AcgErvoQh"},"outputs":[],"source":["plt.figure()\n","plt.plot(history_ens .history[\"val_loss\"], label = \"Val loss\")\n","plt.plot(history_ens .history[\"loss\"], label = \"Train loss\")\n","plt.legend()\n","plt.show()\n","plt.figure()\n","plt.plot(history_ens .history[\"val_accuracy\"], label = \"Val accuracy\")\n","plt.plot(history_ens .history[\"accuracy\"], label = \"Accuracy\")\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","source":["plt.figure()\n","plt.plot(history_ens .history['val_auc'], label = 'val_auc')\n","plt.plot(history_ens .history['auc'], label = 'Train auc')\n","plt.legend()\n","plt.show()\n","\n","plt.figure()\n","plt.plot(history_ens .history['val_tp'], label = 'Val TP')\n","plt.plot(history_ens .history['tp'], label = 'Train TP')\n","plt.plot(history_ens .history['val_fp'], label = 'Val FP')\n","plt.plot(history_ens .history['fp'], label = 'Train FP')\n","plt.legend()\n","plt.show()\n","\n","plt.figure()\n","plt.plot(history_ens .history['val_precision'], label = 'Val Precision')\n","plt.plot(history_ens .history['precision'], label = 'Train Precision')\n","plt.legend()\n","plt.show()\n","\n","plt.figure()\n","plt.plot(history_ens .history['val_recall'], label = 'Val Recall')\n","plt.plot(history_ens .history['recall'], label = 'Train Recall')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"nLNFpuGjVcDj"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1XSeBgGh2gbQCyCLJRRJOmHgPAyPXdS9f","timestamp":1680335055755},{"file_id":"1N9Rvz9GzyY2YLa0jrUvTRXngrJXOCXn4","timestamp":1680270237486}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}